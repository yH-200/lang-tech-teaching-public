{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2019/2020\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Word Sense Induction\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Thursday, 21.11.2019 (23:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Präsenzübung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn  \n",
    "from nltk.book import text2, text3\n",
    "from nltk.text import Text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.cluster import KMeans  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.1:</i> <br>\n",
    "</div>\n",
    "\n",
    "Stopword: Which of the following statements are true?\n",
    "\n",
    "1. A stopword is a word used to stop a parsing process.\n",
    "2. A stopword is a high-frequency word.\n",
    "3. Punctuation marks like ``.``, ``,``, and ``;`` are stopwords.\n",
    "4. \"the, to, and\" are stopwords.\n",
    "5. Stopwords have a maximum length of 3.\n",
    "6. NLTK offers a list of English stopwords through: nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "1. False. A stopword is a frequent word with little lexical content.\n",
    "2. True. Additionally, stopwords ('a', 'the', 'to', 'also') must also carry little lexical content.\n",
    "3. False. First, punctuation marks are not really ''words'', and are thus usually not considered stopwords. In a sense, they are similar to stopwords as they carry no lexical content, i.e. they have no real meaning.\n",
    "4. True\n",
    "5. False. There are stopwords like 'while' or 'will' with length more than 3 characters\n",
    "6. True. There are 127 words in this list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.2:</i> <br>\n",
    "</div>\n",
    "\n",
    "Synset: Which of the following statements are true?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A synset is a set of words that are interchangeable in some context without changing the meaning of a sentence in which they are embedded.\n",
    "2. A synset is a set of all bigrams within WordNet.\n",
    "3. Within WordNet (corpus / lexical ressource) each word corresponds to one or more synsets.\n",
    "4. You may access the synsets of \"dog\" in NLTK by using nltk.corpus.wordnet.synsets('dog')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "1. True. Synset is defined as a set of words grouped by some specific meaning. Since we need only one meaning in the context, the statement is correct.\n",
    "2. False. Synset is a set of words and not bigrams.\n",
    "3. True. A single word may be assigned to different synsets. For example, the word ‘car’ is assigned to such synsets as [‘car’, ‘cable_car’] and [‘car’, ‘elevator_car’].\n",
    "4. True. The output is [Synset(‘dog.n.01’), Synset(‘frump.n.01’), Synset(‘dog.n.03’), Synset(‘cad.n.01’), Synset(‘frank.n.02’), Synset(‘pawl.n.01’), Synset(‘andiron.n.01’), Synset(‘chase.v.01’)]. ‘frump.n.01’ means that ‘dog’ belongs to the synset with the first sense of the noun (‘n’) ‘frump’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.3:</i> <br>\n",
    "</div>\n",
    "\n",
    "There are two possible definitions of 'word':\n",
    "\n",
    "* Word is the same as token, for example ‘see’ and ‘saw’ are different words;\n",
    "* Word is the same as lemma, hence ‘things’ and ‘thing’ become the same word. \n",
    "<br>\n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "\n",
    "1. The lemma of “appeared” is “appear”.\n",
    "2. The lemma of a verb is its infinitive.\n",
    "3. Two words having the same lemma are called homonyms.\n",
    "4. Each token has one and only one lemma.\n",
    "5. Each lemma belongs to one and only one word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "1. True, because ‘appear’ is the normalized form for ‘appeared’.\n",
    "2. True, because the infinitive is the normalized form of a verb.\n",
    "3. False. Homonyms are defined as distinct words that share the same spelling and pronunciation. However, the lemmas of these words may be different. For example, ‘goes’ and ‘went’ have both the lemma ‘go’, but are not homonyms.\n",
    "4. False, as homographs share the same surface form, but might have different lemmas.\n",
    "5. False, because e.g. the lemma ‘be’ corresponds to two words ‘is’ and ‘are’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.4:</i> <br>\n",
    "</div>\n",
    "\n",
    "A stopword list contains high-frequency words like “the”, “to”, “and”, or “also” that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. NLTK provides a predefined list of stopwords: nltk.corpus.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.4.1</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Find all non-stopwords in carroll-alice.txt of the corpus Gutenberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print([word for word in nltk.corpus.gutenberg.words('carroll-alice.txt') if word.lower() not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It iterates over each word in carrol-alice.txt and outputs the word if its lowercase form is not inside the NLTK stopword list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.4.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Implement a function is_the_same_vocab(text1, text2) that compares two texts. It should return true, if the texts have the same vocabulary after removing the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_without_stopwords(text):  \n",
    "    \"\"\"returns the vocabulary of the text without the stopwords\"\"\"\n",
    "    \n",
    "    return set(text.split()) - set(nltk.corpus.stopwords.words('english'))  \n",
    "\n",
    "def is_the_same_vocab(text1, text2):  \n",
    "    \"\"\"check if the texts have the same vocabulary, after removing the stopwords -- works case sensitive\"\"\"\n",
    "    \n",
    "    text1_vocab = get_vocab_without_stopwords(text1)\n",
    "    text2_vocab = get_vocab_without_stopwords(text2)\n",
    "    print(\"Text 1 Vocab:\", text1_vocab, \"\\nText 2 Vocab:\", text2_vocab)\n",
    "    \n",
    "    return(text1_vocab == text2_vocab)  \n",
    "\n",
    "text1 = \"People are hungry before lunch and not hungry after lunch\"  \n",
    "text2 = \"People are not hungry before lunch and hungry after lunch\"  \n",
    "text3 = \"Humans are hungry before dinner and not hungry after dinner\"  \n",
    "\n",
    "print(\"Have same Vocab?\", is_the_same_vocab(text1, text2)) # True  \n",
    "print(\"Have same Vocab?\", is_the_same_vocab(text2, text3)) # False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.5:</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below for some examples on how to work with WordNet. You can also execute it on your computer to see how the output looks like for a better understanding.\n",
    "\n",
    "Explain the code line by line, what datatype and meaning have the methods used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synsets('motorcar'), \"\\n\")\n",
    "print(wn.synset('car.n.01').lemma_names(), \"\\n\")\n",
    "print(wn.synset('car.n.01').definition(), \"\\n\")\n",
    "print(wn.synset('car.n.01').examples(), \"\\n\")\n",
    "print(wn.synset('car.n.01').lemmas(), \"\\n\")\n",
    "print(wn.lemma('car.n.01.automobile'), \"\\n\")\n",
    "print(wn.lemma('car.n.01.automobile').synset(), \"\\n\")\n",
    "print(wn.lemma('car.n.01.automobile').name(), \"\\n\")\n",
    "print(wn.synsets('car'), \"\\n\")\n",
    "print(wn.lemmas('car'), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the synset [Synset('car.n.01')] corresponding to the word 'motorcar'.  \n",
    "print(wn.synsets('motorcar'))  \n",
    "\n",
    "# returns all words ['car', 'auto', 'automobile', 'machine', 'motorcar'] that exist in the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').lemma_names())  \n",
    "\n",
    "# returns the definition 'a motor vehicle with four wheels; usually propelled by an internal combustion engine' \n",
    "# of the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').definition())  \n",
    "\n",
    "# returns the examples ['he needs a car to get to work'] for the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').examples())  \n",
    "\n",
    "# returns all lemmas [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'),\n",
    "# Lemma('car.n.01.motorcar')] that exist in the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').lemmas())  \n",
    "\n",
    "# returns the lemma 'car.n.01.automobile'  \n",
    "print(wn.lemma('car.n.01.automobile'))  \n",
    "\n",
    "# returns the synset 'car.n.01' corresponding to the lemma 'car.n.01.automobile'  \n",
    "print(wn.lemma('car.n.01.automobile').synset())  \n",
    "\n",
    "# returns the name of the lemma 'car.n.01.automobile'  \n",
    "print(wn.lemma('car.n.01.automobile').name())  \n",
    "\n",
    "# returns the synsets [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
    "# corresponding to the word 'car'. We note that word 'car' has more meanings than 'motorcar', \n",
    "# hence more synsets are retrieved from WordNet.  \n",
    "print(wn.synsets('car'))  \n",
    "\n",
    "# returns the lemmas [Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'), Lemma('car.n.04.car'), \n",
    "# Lemma('cable_car.n.01.car')]. 'car.n.0x.car' here are different lemmas that belong to different synsets.  \n",
    "print(wn.lemmas('car')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.6:</i> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "List all the senses of the word \"like\" that you can think of. Now we want to see how many different meanings the word ”like” has with the help of WordNet, using the same operations we used above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('like'):  \n",
    "    print(\"Lemma name:\\n\", synset.lemma_names())  \n",
    "    print(\"Lemma:\\n\", synset.lemmas())  \n",
    "    print(\"Definition:\\n\", synset.definition())  \n",
    "    print(\"POS:\\n:\", synset.pos())\n",
    "    print(\"Example:\\n\", synset.examples(),\"\\n____________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "As you can see above, the words can have different POS tags. Suppose you are only interested in the adjective \"warm\". Write a function that will output definitions and examples of the word \"warm\" under this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('warm', wn.ADJ):  \n",
    "    print(\"Definition:\\n\",synset.definition())\n",
    "    print(\"Example:\\n\",synset.examples(),\"\\n______________\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.3.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "Write functions that outputs the hypernyms and hyponyms of the word 'bank'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"bank\", wn.NOUN): \n",
    "    for l in syn.hypernyms(): \n",
    "            print(syn.name(),\"is a\", l.name(),\"which is\", syn.definition(),\"\\n_______\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"bank\", wn.NOUN): \n",
    "    for l in syn.hyponyms(): \n",
    "            print(syn.name(),\"has subconcept\", l.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.4</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Another aspect we can examine is if a word has an antonym (word with opposite meaning). Write a function that searches an antonym for the word \"like\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"like\"): \n",
    "    for l in syn.lemmas(): \n",
    "        print(syn.name(), \"(\", l.name(), \")\")\n",
    "        if l.antonyms():\n",
    "            print(\"Has the antonym\", [m.name() for m in l.antonyms()], \"\\n\")\n",
    "        else: print(\"Doesn't have an antonym\\n\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.7:</i> <br>\n",
    "</div>\n",
    "\n",
    "The polysemy of a word is the number of senses it has. We now want to compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Try to think of an algorithm that caclulates the ploysemy of a POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "To calculate the average polysemy, we need to count every lemma of the respective POS and the number of meanings the lemma has (size of the synset). Then we divide the total number of meanings for all lemmas of the given POS by the total number of lemmas of the given POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below. Implement it and enhance the main function so that the average polysemy is calculated not only for nouns but also for verbs, adjectives and adverbs (do not implement your algorithm yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgPolysemy():\n",
    "    #TODO, return average polysemy\n",
    "    \n",
    "avgN = avgPolysemy()\n",
    "    #TODO\n",
    "\n",
    "print(\"Average polysemy of nouns: \", avgN)\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgPolysemy(pos):\n",
    "    nrOfSynsets = 0\n",
    "    nrOfLemmas = 0\n",
    "    lemmas = []\n",
    "    #TODO\n",
    "\n",
    "avgN = avgPolysemy(wn.NOUN)\n",
    "avgV = avgPolysemy(wn.VERB)\n",
    "avgADJ = avgPolysemy(wn.ADJ)\n",
    "avgADV = avgPolysemy(wn.ADV)\n",
    "\n",
    "print(\"Average polysemy of nouns:\", avgN)\n",
    "print(\"Average polysemy of verbs:\", avgV)\n",
    "print(\"Average polysemy of adjectives:\", avgADJ)\n",
    "print(\"Average polysemy of adverbs:\", avgADV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.3</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Now implement your algorithm and calculate the average polysemy for nouns, verbs, adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgPolysemy(pos):\n",
    "    nrOfSynsets = 0\n",
    "    nrOfLemmas = 0\n",
    "    lemmas = []\n",
    "    \n",
    "    for synset in wn.all_synsets(pos):\n",
    "        for lemma in synset.lemmas():\n",
    "            lemmas.append(lemma.name())\n",
    "    lemmas = set(lemmas)\n",
    "    \n",
    "    for lemma in lemmas:\n",
    "        count = len(wn.synsets(lemma, pos))\n",
    "        if count > 0:\n",
    "            nrOfLemmas += 1\n",
    "            nrOfSynsets += count\n",
    "            \n",
    "    return nrOfSynsets/nrOfLemmas\n",
    "\n",
    "avgN = avgPolysemy(wn.NOUN)\n",
    "avgV = avgPolysemy(wn.VERB)\n",
    "avgADJ = avgPolysemy(wn.ADJ)\n",
    "avgADV = avgPolysemy(wn.ADV)\n",
    "\n",
    "print(\"average polysemy of nouns: \", avgN)\n",
    "print(\"average polysemy of verbs: \",  avgV)\n",
    "print(\"average polysemy of adjectives: \",  avgADJ)\n",
    "print(\"average polysemy of adverbs: \",  avgADV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.8:</i> <br>\n",
    "</div>\n",
    "\n",
    "A concordance view shows us every occurrence of a given word, together with some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.8.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Extract context samples of word ‘affection’ in “Sense and Sensibility” (text2) corpus of nltk and word ‘lived’ in “Book of Genesis” (text3) corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.book.text2.concordance(\"affection\")\n",
    "print()\n",
    "nltk.book.text3.concordance(\"lived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.8.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Similar words for the term can be found if words share same context as that of token. \n",
    "\n",
    "For example: monsterous occurred in contexts such as the ___ pictures and a ___ size . What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:\n",
    "\n",
    "Find out similar words for affection in “Sense and Sensibility” (text2) corpus of nltk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.book.text2.similar(\"affection\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering - Tf-Idf Vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.9:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Extract all the context of term affection in the “Sense and Sensibility” (text2) corpus of nltk with a window size of 21 (each context should have 10 words before and after the term). Provide the total number of contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nltk.ConcordanceIndex(nltk.book.text2.tokens, key=lambda s: s.lower())\n",
    "window_size = 10\n",
    "affection_contexts = []\n",
    "for index in c.offsets('affection'):  \n",
    "    if index > 10:\n",
    "           #TODO\n",
    "print(len(affection_contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nltk.ConcordanceIndex(nltk.book.text2.tokens, key=lambda s: s.lower())  \n",
    "window_size = 10  \n",
    "affection_contexts = []  \n",
    "for index in c.offsets('affection'):  \n",
    "    if index > 10:  \n",
    "        affection_contexts.append(nltk.book.text2.tokens[index - window_size:index + window_size])  \n",
    "print(len(affection_contexts))  # return 79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Preprocess the contexts by removing punctuation and stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))  \n",
    "affection_contexts_no_stopwords = []  \n",
    "for context in affection_contexts:  \n",
    "    # remove punctuation\n",
    "    #TODO\n",
    "    \n",
    "    # remove stopwords  \n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))  \n",
    "affection_contexts_no_stopwords = []  \n",
    "for context in affection_contexts: \n",
    "    # remove punctuation \n",
    "    context = [word for word in context if word not in string.punctuation]\n",
    "    # remove stopwords  \n",
    "    context_no_stopwords = []  \n",
    "    for token in context:\n",
    "        if token not in stopwords:  \n",
    "            context_no_stopwords.append(token)  \n",
    "    affection_contexts_no_stopwords.append(' '.join(context_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.3.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Generate a tf-idf feature for the preprocessed text using sci-kit learn feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()  \n",
    "X_tf_idf = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()  \n",
    "X_tf_idf = vectorizer.fit_transform(affection_contexts_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.4.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Apply k means algorithm with k = 3(number of cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 3 # number of clusters  \n",
    "\n",
    "# Apply algorithm \n",
    "model_tf_idf = #TODO\n",
    "\n",
    "# Training the algorithm\n",
    "#TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 3 # number of clusters  \n",
    "model_tf_idf = KMeans(n_clusters=true_k) # apply algorithm  \n",
    "model_tf_idf.fit(X_tf_idf) # training the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.5.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Print the top-ten terms that represent each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model_tf_idf.cluster_centers_.argsort()[:, ::-1]  # getting indexes of centroids   \n",
    "terms = vectorizer.get_feature_names() # getting feature terms   \n",
    "for i in range(true_k):  \n",
    "    print(\"Cluster %d:\" % i)\n",
    "    \n",
    "    # get top 10 terms\n",
    "    \n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model_tf_idf.cluster_centers_.argsort()[:, ::-1]  # getting indexes of centroids   \n",
    "terms = vectorizer.get_feature_names() # getting feature terms   \n",
    "for i in range(true_k):  \n",
    "    print(\"Cluster %d:\" % i)  \n",
    "    for ind in order_centroids[i, :10]:  \n",
    "        print(\" %s\" % terms[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.9.6.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "Assign context to the corresponding cluster predicted by the k-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}  \n",
    "for context in affection_contexts:  \n",
    "    X = vectorizer.transform([context])  # get tf-idf vector for the context  \n",
    "    predicted = model_tf_idf.predict(X)  # predict the cluster  \n",
    "    \n",
    "    # generate clusters\n",
    "    \n",
    "    # TODO    \n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}  \n",
    "for context in affection_contexts:  \n",
    "    X = vectorizer.transform([' '.join(context)])   # get tf-idf vector for the context  \n",
    "    predicted = model_tf_idf.predict(X)[0]          # predict the cluster  \n",
    "    if predicted not in clusters.keys():    \n",
    "        clusters[predicted] = []  \n",
    "    clusters[predicted].append(context)   \n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.10:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Import common_text corpus from gensim library and display first 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Train the Word2vec model based on gensim with the follwing parameters:\n",
    "    - Size of the dimension = 100\n",
    "    - Context window size = 5\n",
    "    - Minimum frequency count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(common_texts, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.3.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Display the vector for the word \"computer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.4.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Compute the similarity score between the words 'graph' and 'trees'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('graph', 'trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.5.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Calculate the top 5 most similar word to the word 'graph'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('graph')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pretrained google word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.11:</i> <br>\n",
    "</div>\n",
    "\n",
    "The Google word2vec embeddings was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GoogleNews-vectors-negative300.bin.gz embeddings in your current working directory and unzip it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a 1.53 Gigabytes file. You can download it from here:\n",
    "\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.11.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Generate gensim word2vec model using google pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec_model = #TODO # load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.11.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Operation (king – man) + woman = ? can be performed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the (Germany – Berlin) + Moscow = ? operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_word2vec_model.most_similar(positive=['Moscow', 'Germany'], negative=['Berlin'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.1:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.1.1.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "Try K-means clustering with genism word2vec embeddings features instead of tf-idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec #get gensim word2vec feature extraction utility  \n",
    "affection_contexts_no_stopwords = [context.split(' ') for context in affection_contexts_no_stopwords]  \n",
    "\n",
    "# Apply word2vec algorithm\n",
    "\n",
    "w2v = #TODO\n",
    "\n",
    "X_w2v = w2v[w2v.wv.vocab]  \n",
    "\n",
    "\n",
    "model_w2v = KMeans(n_clusters=true_k) \n",
    "\n",
    "# Train kmeans algorithm\n",
    "#TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.1.2.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "Try K-means clustering with genism word2vec embeddings features instead of tf-idf features. Evaluate the performance of the k-means algorithm using silhouette metric for embedding and tf-idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score  \n",
    "\n",
    "model_w2v = KMeans(n_clusters=true_k)  \n",
    "model_w2v.fit(X_w2v)  \n",
    "\n",
    "# print Silhouette scores\n",
    "\n",
    "# TODO "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
