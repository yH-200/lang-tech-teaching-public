{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2019/2020\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Named Entity / Coreference\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Thursday, 14.11.2019 (23:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Präsenzübung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.corpus import*\n",
    "from nltk.book import*\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import gazetteers, names\n",
    "\n",
    "from sklearn import datasets, svm, tree, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 4.1:</i> Named Entity Recognition: <br>\n",
    "</div>\n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "1. The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities.\n",
    "2. Named entity recognition is a method to extract person names from text.\n",
    "3. Named entities are language independent.\n",
    "4. In named entity recognition we need to be able to identify the beginning and the end of multi-token sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. True, a named entity recognition (NER) system should identify all textual mentions of the named entities.\n",
    "2. True. PERSON is a valid named entity type; however, the NER systems may extract numerous other types as LOCATION or ORGANIZATION.\n",
    "3. False. Although names of persons are usually not translated (besides the alphabet), currencies and dates can be specific to one language. Countries (GPE: geo-political entities) are sometimes translated (e.g. Elfenbeinküste - Ivory coast - Cote d'Ivoire) \n",
    "4. True, since named entities may consist of more than one word (e.g. George W. Bush)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Named Entities Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A type of noun phrase that is of particular interest is a named entity. This might be a person, such as Albert Einstein, or a place, such as Duisburg or a business, such as Irish Pub. <br>\n",
    "In general, this is a hard problem. Words can have multiple uses, and there’s an unbounded number of possible names. Within a domain, though, we can have better luck. NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk() <br>\n",
    "The table below states the commonly used types of named entities, as they are provided by nltk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| NE Type | Examples  |\n",
    "|------|------|\n",
    "|ORGANIZATION|Georgia-Pacific Corp., WHO\n",
    "|PERSON| Eddy Bonte, President Obama\n",
    "|LOCATION|Murray River, Mount Everest\n",
    "|DATE|June, 2008-06-29\n",
    "|TIME|two fifty a m, 1:30 p.m.\n",
    "|MONEY|175 million Canadian Dollars, GBP 10.40\n",
    "|PERCENT|twenty pct, 18.75%\n",
    "|FACILITY|Washington Monument, Stonehenge\n",
    "|GPE (geo-political entities)|South East Asia, Midlothian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 4.2:</i> <br>\n",
    "</div>\n",
    "\n",
    "Use the sentence below: <br><br>\n",
    "The capital of the United States of America is named after the first US president George Washington."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.2.1</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Use word_tokenize to tokenize the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "We tokenize the above text using the expression nltk.word_tokenize(sentence). The tokenizer outputs the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(\"The capital of the United States of America is named after the first US president George Washington.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.2.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Use nltk.pos_tag to tag the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "We assign part-of-speech tags to the above text using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nltk.word_tokenize(\"The capital of the United States of America is named after the first US president George Washington.\")\n",
    "sentence = nltk.pos_tag(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.2.3</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Use nltk.ne_chunk to chunk the tagged sentence. Experiment with the argument \"binary\". What is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "We extract named entities from the above text using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nltk.ne_chunk(sentence, binary=False)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that setting the 'binary' parameter to True outputs named entities without types. For example the excerpt \"('America','NNP')\" labels America as a Named Entity. Whereas, the default setting of the 'binary' parameter to False enables the system to output Named Entity types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.2.4</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Draw (.draw()) and analyze the resulting tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nltk.ne_chunk(sentence, binary=True)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the named entities are extracted: United States, America, US and George Washington. <br>\n",
    "Also the tree structure is very flat, with the bottom layer of extracted entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.2.5</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Write a function extract_entity_names(tree), that extracts all identified named entities of the given tree and returns it as a list of words.<br>\n",
    "Since 'tree' is is a nested structure implement this function using a recursion. It is standard to use a recursive function to traverse a tree. The listing below defines an algorithm to traverse a tree. You may change it to fit your purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(t):\n",
    "    try:\n",
    "        t.label\n",
    "    except AttributeError:\n",
    "        print(t)\n",
    "    else:\n",
    "        #Now we know that t.node is defined\n",
    "        print('(', t.label),\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "        print(')'),\n",
    "\n",
    "traverse(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "Refer to the code in the following listing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_names(tree):\n",
    "    '''This function extracts all named entities from a tree. \n",
    "    If a tree node has the label 'NE' it extracts all values of the leaves and adds it to the set of named entities\n",
    "    This named entity is coverted to a list and returned'''\n",
    "    \n",
    "    entity_names = []\n",
    "    try:\n",
    "        if tree.label() == 'NE':\n",
    "            entity = ' '.join(leaf[0] for leaf in tree.leaves())\n",
    "            entity_names.append(entity)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        for child in tree:\n",
    "            entity = extract_entity_names(child)\n",
    "            if len(entity)>0:\n",
    "                entity_names.append(entity)\n",
    "    return entity_names\n",
    "    \n",
    "\n",
    "text = \"The capital of the United States is named after the first US president George Washington\"\n",
    "\n",
    "sentence = nltk.word_tokenize(text)\n",
    "sentence = nltk.pos_tag(sentence)\n",
    "tree = nltk.ne_chunk(sentence, binary = True)\n",
    "\n",
    "print (\"Tree: \\n\",tree)\n",
    "print (\"Named entities: \",extract_entity_names(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, F-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 4.3:</i> <br>\n",
    "</div>\n",
    "\n",
    "The following confusion matrix shows the evaluation result of a named entities classifier. The columns contain the gold standard and the rows the system output. The target class is NE.\n",
    "\n",
    "Confusion Matrix |NE | no NE |\n",
    "-|-|-|\n",
    "NE| 50 | 30 |\n",
    "no NE| 20 | 200 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.3.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "How many true positives, true negatives, false positives and false negatives are there? How do you interpret them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "There are 50 true positives (System and Gold say \"NE\"), and 200 true negatives (System and Gold say \"no NE\"): These are the cases where the system made the correct decision.\n",
    "\n",
    "There are 30 false positives (System says \"NE\" but correct would be \"no NE\"), and 20 false negatives (System says \"no NE\" but correct would be \"NE\"): These are the cases where the system made an incorrect decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.3.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Compute precision, recall and F-score given the confusion matrix above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 50/(50+30)\n",
    "recall = 50/(50+20)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score: \", 2*precision*recall/(precision+recall) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your own Named Entities Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 4.4:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.4.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "What does the following code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NER_clean.csv\", delimiter = \"\\t\", encoding=\"utf-8\", names=[\"WORD\", \"NE\"], quoting=3)\n",
    "df[\"WORD\"] = df[\"WORD\"].apply(str)\n",
    "print(df[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "A csv file is read in and converted to a data frame with two columns: One contains the word and the other one a binary distinction whether the word is a named entity (\"True\") or not (\"False\"). (quoting = 3 is necessary to also read in quotation marks as regular tokens).\n",
    "The second line of code only makes sure that anything in the first column is treated as a string (even numbers). Finally, the first 30 rows of the data frame are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.4.2</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "What does the following code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(df.loc[:, \"WORD\"])\n",
    "df[\"WORDLENGTH\"] = [len(word) for word in words]\n",
    "print(df[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "    \n",
    "A third column is added which contains the length the token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.4.3</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Add 4 columns to the data frame which contain\n",
    "\n",
    "- whether a word is capitalized (True/False)\n",
    "- whether a word is fully written in uppercase (True/False)\n",
    "- whether the word is a noun (True/False)\n",
    "- whether the word appears in the corpus \"names\" or \"gazetteers\" from NLTK (True/False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IS_CAPITALIZED\"] = [word.istitle() for word in words]\n",
    "df[\"IS_UPPER\"] = [word.isupper() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos_tag(words, tagset=\"universal\")\n",
    "df[\"NOUN\"] = [tag == \"NOUN\" for (word, tag) in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_gaz = set(gazetteers.words())\n",
    "set_names = set(names.words())\n",
    "df[\"IN_LIST\"] = [word in set_gaz or word in set_names for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">4.4.4</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "The following code creates a Decision Tree Classifier which classifies whether a token is a named entity or not based on the features you provided above. The data are split in a training and a test set and the variable \"predicted\" contains the predicted labels (NE = \"True\", no NE =\"False\") while \"gold\" contains the corresponding gold labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, 2:len(df.columns)]\n",
    "y = df.iloc[:, [1]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "c_tree = DecisionTreeClassifier(max_depth=4)\n",
    "c_tree.fit(x_train, y_train)\n",
    "\n",
    "predicted = list(c_tree.predict(x_test))\n",
    "gold = list(y_test.loc[:, \"NE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the predicted and the gold labels, compute precision, recall and F-score for the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = sum(pair == (True, True) for pair in zip(predicted, gold))\n",
    "fp = sum(pair == (True, False) for pair in zip(predicted, gold))\n",
    "fn = sum(pair == (False, True) for pair in zip(predicted, gold))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "fscore = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\", recall)\n",
    "print(\"f-score:\", fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare your results to the built-in classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gold,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Guidlines\n",
    "* The submission has to be done by a team of two people. **Individual submissions will not be graded.**\n",
    "* Please state **the name and matriculation number of all team members in every submission** clearly.\n",
    "* Only **one team member** should submit the homework. If more than one version of the same homework is submitted by accident (submitted by more than one group member), please reach out to a tutor **as soon as possible**. Otherwise, the first submitted homework will be graded.\n",
    "* The submission must be in a Jupyter Notebook format (.ipynb). Submissions in other formats will **not be graded**.\n",
    "* It is not necessary to also submit the part of the exercise discussed by the tutor, please only submit the homework part.\n",
    "* If pictures need to be submitted, it is allowed to hand them in in a zip folder, together with the notebook. They should be added to the notebook like this: <br> *!\\[example1](examplepicture1.PNG)* (without apostrophs in a Markdown-Cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 4.1:</i> \n",
    "</div>\n",
    "\n",
    "<i class=\"subtask\">4.1.1</i> \n",
    "Annotate all named entities in the file \"Langtech_NER.txt\"\n",
    "\n",
    "* The file contains 100 German sentences (note that the sentences do not form a coherent text) and each sentence may contain one or more named entities but it is also possible that there is no named entity in a sentence\n",
    "<br><br>\n",
    "* The 4 named entity types to annotate are PERSON (PER), ORGANIZATION (ORG), LOCATION (LOC), OTHER (OTH). For further information about which named entity belongs to which type, please refer to the \"NoSta-D-TagSet\" on page 6 in the file \"Clarin_NoSta-D_NER-AnnotationGuidelines.pdf\" that you can download from Moodle. Important: you are not asked to follow these annotation guidelines completely. Especially, note the followning:\n",
    "     * Anything that is tagged with \"deriv\" or \"part\" tags according to these guidelines is ignored (e.g. LOCderiv, ORGpart)\n",
    "     * In our annotation, there are no nested named entities. For example \"Bayern München\" is labeled as ORG and the individual parts \"Bayern\" and \"München\" are not labeled as LOC. As a general rule, the longest possible span gets the label.\n",
    " <br>\t\n",
    "<br>\n",
    "* Upload the annotated file (ending with \".ann\", see below), to Moodle. Make sure that the filename contains your name!\n",
    "\n",
    "<i class=\"subtask\">4.1.2</i> \n",
    "\n",
    "Write down at least 5 different cases that you found difficult to annotate. For each, write down 1-2 sentences explaining why it was difficult (e.g. by saying which other label could have applied and why or why you were unsure whether something is a named entity or not). Upload your descriptions to Moodle as a PDF file.\n",
    "\n",
    "\n",
    "\n",
    "### Technical instructions\n",
    "\n",
    "\n",
    "- Download the annotation tool YEDDA from https://github.com/jiesutd/YEDDA\n",
    "\n",
    "- Attention: YEDDA requires Python 2.7, so make sure you have this version installed!\n",
    "\n",
    "- To start the annotation, open a console (in the YEDDA-master folder) and type python YEDDA.py (make sure you start it with Python 2 not Python 3, so maybe you have to type something like /path/to/python2 YEDDA.py !)\n",
    "\n",
    "- Download the file \"Langtech_NER.config\" from Moodle and place it in the folder \"YEDDA-master/configs/\".\n",
    "\n",
    "- To open the sentences to annotate, click on \"open\" and select the file \"Langtech_NER.txt\" (or \"Langtech_NER.ann\" if you have already saved an annotated version and want to continue)\n",
    "\n",
    "- Select the correct set of labels: In the drop down menu under \"Map Templates\" on the right hand side, select the file \"Langtech_NER.config\"\n",
    "\n",
    "- To annotate a named entity, mark the whole Named Entity and press the key on the keyboard that is associated with the right label (A: PERSON, B: ORGANISATION, C:LOCATION, D:OTHER)\n",
    "\n",
    "- To change a label, click within the entity span and press the key for the new label\n",
    "\n",
    "- To remove a label, click within the entity span and press 'q' . Important: In order to remove the label, do not mark the whole entity. If you then press 'q' this will remove the whole entity, not just the label!\n",
    "\n",
    "- Clicking on \"Export\" will save the annotated text. Note: two files will be saved, one ending with *.ann and one with *.anns. Upload the one ending with *.ann to Moodle (change the filename so that it contains your name!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
